{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3831af93-f396-46b7-acbc-345a6afed06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de27de3c-79f7-42b3-8444-74cdeb010d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnxruntime\n",
      "  Using cached onnxruntime-1.18.0-cp310-cp310-win_amd64.whl.metadata (4.4 kB)\n",
      "Collecting coloredlogs (from onnxruntime)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime) (24.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime) (5.27.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime) (1.12.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime)\n",
      "  Using cached pyreadline3-3.4.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Using cached onnxruntime-1.18.0-cp310-cp310-win_amd64.whl (5.6 MB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached pyreadline3-3.4.1-py3-none-any.whl (95 kB)\n",
      "Installing collected packages: pyreadline3, flatbuffers, humanfriendly, coloredlogs, onnxruntime\n",
      "Successfully installed coloredlogs-15.0.1 flatbuffers-24.3.25 humanfriendly-10.0 onnxruntime-1.18.0 pyreadline3-3.4.1\n",
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "#!pip install transformers\n",
    "import transformers\n",
    "#!pip install pytesseract\n",
    "import pytesseract\n",
    "#!pip install pdf2image\n",
    "from pdf2image import convert_from_path\n",
    "import nltk\n",
    "#!pip install onnx\n",
    "import onnx\n",
    "#!pip install onnxruntime\n",
    "import onnxruntime as ort\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6598c1-f0d9-4881-8b3d-fbcc950f0bc7",
   "metadata": {},
   "source": [
    "## Convert PDFs to text using OCR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2297f75-2693-4567-b67c-e3b879f26ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def pdf_to_text(file_path):\n",
    "    images = convert_from_path(file_path)\n",
    "    text = \"\"\n",
    "    for image in images:\n",
    "        text += pytesseract.image_to_string(image)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a611e274-ea44-4a1a-aced-2c3ad1e70275",
   "metadata": {},
   "source": [
    "##  Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24c65c2c-052b-4c3d-806e-58ca16db5a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def annotate_text(text):\n",
    "    annotations = {}\n",
    "    annotations['invoice_number'] = re.search(r'Facture Num√©ro: (\\d+)', text).group(1)\n",
    "    annotations['date'] = re.search(r'Date: (\\d{4}-\\d{2}-\\d{2})', text).group(1)\n",
    "    annotations['company'] = re.search(r'Entreprise: ([\\w\\s]+)', text).group(1)\n",
    "    annotations['amount'] = re.search(r'Montant: (\\d+ EUR)', text).group(1)\n",
    "    return annotations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c5495b-1d1c-4a35-b585-ef14e01386fd",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7188c03-6be7-49c3-9902-0d4b8896f269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b97aa81adab4b34987f4411cef68552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/456 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\USER\\.cache\\huggingface\\hub\\models--dbmdz--bert-base-german-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e332f4932218458cad7c88c46d646733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/442M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dbmdz/bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "899731fb9e764a61b8949311b1a0247b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a611f0efe981405f9e65ce5079e6d565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/240k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "model_name = \"dbmdz/bert-base-german-cased\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Fine-tune the model with your dataset...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07b3f69-9b00-4c85-ba72-9ef1f8ecdf65",
   "metadata": {},
   "source": [
    "## Model Optimization\n",
    "## 1. Convert to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07b24d6d-0741-4ad5-810a-bb3895ea33b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnx\n",
    "\n",
    "dummy_input = torch.zeros(1, 512, dtype=torch.int64)\n",
    "torch.onnx.export(model, dummy_input, \"model.onnx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30094f43-8ca6-4f3c-bc8a-5831718f29d1",
   "metadata": {},
   "source": [
    "## 2. Optimize the Model\n",
    "# Use quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a5205e2-d004-4268-844b-db5bceea6fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "quantize_dynamic(\"model.onnx\", \"model_quantized.onnx\", weight_type=QuantType.QUInt8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b84eaf-674d-4418-9c62-c9c0fd867780",
   "metadata": {},
   "source": [
    "## 3: Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797dc19e-d2ff-4bdc-b08d-9fc227af00f9",
   "metadata": {},
   "source": [
    "## Write a script to load and run the optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74ceb010-1aa8-4df2-b142-a55ee3425566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "ort_session = ort.InferenceSession(\"model_quantized.onnx\")\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "def run_model(input_text):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(inputs['input_ids'])}\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "    return ort_outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449aa731-4c98-4f3f-a5fa-74e62857a269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
